{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch accelerate gradio --quiet"
      ],
      "metadata": {
        "id": "QrzVkju3_m2m"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv-mh18biM2n",
        "outputId": "5b3ef3fa-c980-4812-93e8-a1edb9c2b7bf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "7c8owBpdiEp4"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "DMZYQpon_lgg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "data_dir = \"/content/pubmed_20k\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "url_train = \"https://huggingface.co/datasets/armanc/pubmed-rct20k/resolve/main/train.jsonl\"\n",
        "file_train = os.path.join(data_dir, \"pubmed-20k-train.jsonl\")\n",
        "\n",
        "if not os.path.exists(file_train):\n",
        "    print(\"Downloading PubMed 20k RCT train dataset...\")\n",
        "    r = requests.get(url_train, stream=True)\n",
        "    with open(file_train, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(\"Dataset already exists!\")"
      ],
      "metadata": {
        "id": "0mhdlxaF_rjn",
        "outputId": "68799e31-d240-49a3-ad6d-412e9b886fe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove placeholder\n",
        "    text = re.sub(r'[#@<>/▃]', '', text)\n",
        "    # remove space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'(\\d)\\s+\\.', r'\\1.', text)\n",
        "    text = re.sub(r'\\.\\s+(\\d)', r'.\\1', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def parse_pubmed_rct_jsonl(path):\n",
        "    texts = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            cleaned = clean_text(data[\"text\"])\n",
        "            texts.append(cleaned)\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "0IiIAJT7_t8J"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"allenai-specter\", device='cuda')\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb0M5dgPkpO_",
        "outputId": "4dca62da-2be0-405b-894b-8e2df29a19ec"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_sample = texts[:32]  #only 32 paragraph\n",
        "\n",
        "embeddings = []\n",
        "batch_size = 8\n",
        "\n",
        "for i in range(0, len(texts_sample), batch_size):\n",
        "    batch_texts = texts_sample[i:i+batch_size]\n",
        "\n",
        "    # create embedding\n",
        "    batch_emb = embedding_model.encode(batch_texts, convert_to_tensor=True, device='cuda')\n",
        "\n",
        "    #  diffusion\n",
        "    batch_emb_denoised = diffusion_denoise(batch_emb)\n",
        "\n",
        "    embeddings.append(batch_emb_denoised)\n",
        "\n",
        "embeddings = torch.cat(embeddings)\n",
        "print(\"Embeddings created with diffusion denoising\")\n",
        "print(\"Device of embeddings:\", embeddings.device)\n",
        "print(\"Shape of embeddings:\", embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbeBTjwGkxng",
        "outputId": "75c811ab-5734-4db4-e59c-4cd158e168d4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings created with diffusion denoising\n",
            "Device of embeddings: cuda:0\n",
            "Shape of embeddings: torch.Size([32, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_np = embeddings.detach().cpu().numpy()\n",
        "dim = emb_np.shape[1]\n",
        "\n",
        "# create index CPU\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(emb_np)\n",
        "\n",
        "print(\"VectorDB ready on CPU\")\n",
        "print(\"Number of vectors in index:\", index.ntotal)"
      ],
      "metadata": {
        "id": "pQ1eApsS_0rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93183482-9a42-4876-ef2d-a62e14864662"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorDB ready on CPU\n",
            "Number of vectors in index: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrCJJ38MqVpf",
        "outputId": "56061706-088e-4b5c-8b67-9b507fca1755"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "llm_model_name = \"microsoft/BioGPT-Large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(llm_model_name).to('cuda')\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "#  test\n",
        "prompt = \"Describe the function of the liver in humans.\"\n",
        "output = generator(prompt, max_length=100, do_sample=True)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Axz2tjAI_2Zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4728083-624c-466c-f3ba-20395f4a966a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe the function of the liver in humans. < / FREETEXT > < / TITLE > ▃\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_medical(query, top_k=3):\n",
        "    # gpu on embedding query\n",
        "    q_emb = embedding_model.encode([query], convert_to_tensor=True, device='cuda')\n",
        "    q_emb = diffusion_denoise(q_emb)\n",
        "\n",
        "    # VectorDB (CPU index)\n",
        "    D, I = index.search(q_emb.cpu().numpy(), top_k)\n",
        "    retrieved_texts = [texts[i] for i in I[0]]\n",
        "\n",
        "    # prompt\n",
        "    prompt = (\n",
        "        \"Relevant medical excerpts (PubMed 20k RCT):\\n\\n\" +\n",
        "        \"\\n\".join(retrieved_texts) +\n",
        "        \"\\n\\nGenerate a clear research summary or explanation (educational only, no medical advice):\\n\"\n",
        "    )\n",
        "\n",
        "    # LLM\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_length=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    return output\n",
        "\n",
        "# test\n",
        "query = \"Latest treatments for type 2 diabetes complications\"\n",
        "print(rag_medical(query))\n"
      ],
      "metadata": {
        "id": "xEuV7IZf_7yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc32c4b-24f0-467d-f4b7-6e9c93af62ea"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant medical excerpts (PubMed 20k RCT):\n",
            "\n",
            "Further , there was a clinically relevant reduction in the serum levels of IL- , IL- , TNF - , and hsCRP at weeks in the intervention group when compared to the placebo group .\n",
            "The mean difference between treatment arms ( % CI ) was ( - ) , p ; ( - ) , p ; ( - ) , p ; and ( - ) , p , respectively .\n",
            "These differences remained significant at weeks .\n",
            "\n",
            "Generate a clear research summary or explanation (educational only, no medical advice):\n",
            " Mean difference between treatment arms (% CI) was –, p; (-), p; (-), p; and (-), p, respectively. For all the values, p < 0. 0 5. < / FREETEXT > < / PARAGRAPH > ▃ < PARAGRAPH > < FREETEXT > Relevance statement: The mean differences between treatment arms (% CI) were (-), p; (-), p; (-), p; and (-), p, respectively. For all the values, p < 0. 0 5. < / FREETEXT > < / PARAGRAPH > ▃ < PARAGRAPH > < FREETEXT > Conclusion: The present study demonstrates that a structured education programme is an effective strategy for the management of RA, with a similar impact on disease activity, disability, and quality of life when compared to a structured education programme with medical advice. < / FREETEXT > < / PARAGRAPH > ▃ < PARAGRAPH > < FREETEXT > Competing interests None. < / FREETEXT > < / PARAGRAPH > ▃ < PARAGRAPH > < FREETEXT > Reference < / FREETEXT > < / PARAGRAPH > ▃ < PARAGRAPH > < FREETEXT > Antoine Drouet 1, Jean-Louis Fauquet 1, Yves-Marie Lebrun 1, Henri-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio\n",
        "def ui(query):\n",
        "    return rag_medical(query)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=ui,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a medical research query...\"),\n",
        "    outputs=\"textbox\",\n",
        "    title=\"Medical RAG + GenAI + Diffusion\",\n",
        "    description=\"Search PubMed 20k RCT using embeddings, diffusion, and LLM (educational only).\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "FHXm9zUC_9sO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "9640e716-47ce-4ff2-c2ed-8908a107d66a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://654da9312cf20c94dc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://654da9312cf20c94dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}